{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow code for a Multilayer Perceptron\n",
    "\n",
    "In this section we will go through the code for a multilayer perceptron in TensorFlow.\n",
    "\n",
    "Built around the implementation by [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we set up the required imports and define the location of the mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../scratch/\", one_hot=True) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the relevant network parameters and graph input for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "learning_rate = 0.001 # Initial learning rate\n",
    "training_epochs = 50 # Number of epochs to train\n",
    "batch_size = 100 # Number of images per batch\n",
    "display_step = 2 # How often to output model metrics during training\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input placeholders\n",
    "X = tf.placeholder(\"float\", [None, n_input], name='X') # Input data\n",
    "Y = tf.placeholder(\"float\", [None, n_classes], name='Y') # Input labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise weights and biases for the network.\n",
    "\n",
    "We are giving weights to every feature (in the first layer this is for each pixel, in the second layer this is every feature we extracted in the first layer). These weights are to inform the model how important the features are in making up the next set of features. As we don't actually know how important each feature is yet, we initialise with  values randomly drawn from a normal distribution with mean 0 and variance 1.\n",
    "\n",
    "Biases are additional constants attached to neurons and added to the weighted input before the activation function is applied.\n",
    "\n",
    "Generally, the features in the hidden layers are not easily discoverable. These hidden layers are considered something of a black-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='h1'),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='h2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name='h_out')\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='b_out')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "The model is ‘multi-layer’ because there is more than one hidden layer, as below we define `layer_1` and `layer_2`.\n",
    "\n",
    "The MLP definition below does two things:\n",
    "\n",
    "1. It defines the model in model_perceptron()\n",
    "2. It initialises and assigns values to each layer of the network as follows: (input, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'], name='layer_1')\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'], name='layer_2')\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out'], name='out_layer') + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we construct a model object, passing the X placeholder as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer\n",
    "\n",
    "In the following snippet we define our loss operation, optimiser and initialise our global variables.\n",
    "\n",
    "`tf.reduce_mean` - Computes the mean of elements across dimensions of a tensor.\n",
    "\n",
    "`tf.train.AdamOptimizer` - Adam optimiser provides an adaptive gradient algorithm.\n",
    "\n",
    "`optimizer.minimize` - Takes care of both computing the gradients and applying them with respect to `loss_op`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y), name='loss_op')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, name='train_op')\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(logits, name='prediction') # Raw prediction values\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1), name='correct_pred') # Predictions that are correct\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy') # Calculate accuracy\n",
    "# Define writer for tensorbord log output\n",
    "writer = tf.summary.FileWriter(os.path.join(os.getcwd(),\"mlp-tb-\" + str(datetime.fromtimestamp(time())) ), graph=tf.get_default_graph())\n",
    "\n",
    "# Define and name tensorboard histograms\n",
    "tf.summary.histogram(\"loss\", loss_op)\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "#tf.summary.scalar(\"loss\", loss_op)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "#tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Merge all summaries into a single output\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # Get number of batches\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size) # Train batch\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per display_step\n",
    "        if epoch % display_step == 0:\n",
    "            loss, acc, summary = sess.run([loss_op, accuracy, merged_summary_op], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            writer.add_summary(summary, epoch) # Write current step output to tensorboard\n",
    "            \n",
    "            #print(\"loss \", loss, \"\\nacc \", acc,\"\\nsummary \", summary)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    \n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # Build confusion matrix from ground truth labels and model predictions\n",
    "    conf_mat = tf.confusion_matrix(tf.argmax(Y, 1),tf.argmax(pred, 1)).eval({X: mnist.test.images, Y: mnist.test.labels})\n",
    "    %matplotlib inline\n",
    "    # Plot matrix\n",
    "    plt.matshow(conf_mat)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Real Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tensorboard using an ngrok tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import signal\n",
    "\n",
    "def get_process_pid(pstring):\n",
    "    pid = None\n",
    "    for line in os.popen(\"ps ax | grep \" + pstring + \" | grep -v grep | grep -v defunct\"):\n",
    "        fields = line.split()\n",
    "        pid = fields[0]\n",
    "    return pid\n",
    "\n",
    "LOG_DIR = os.getcwd()\n",
    "NG_DIR = LOG_DIR\n",
    "# Uncomment if running locally\n",
    "#NG_DIR = os.path.dirname(LOG_DIR)\n",
    "NG_ZIP = os.path.join(NG_DIR, 'ngrok-stable-linux-amd64.zip')\n",
    "NG_BIN = os.path.join(NG_DIR, 'ngrok')\n",
    "\n",
    "# Download ngrok binary\n",
    "if not os.path.isfile(NG_ZIP):\n",
    "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip \\\n",
    "        -P {NG_DIR}\n",
    "if not os.path.isfile(NG_BIN):        \n",
    "    !unzip -o {NG_DIR}/ngrok-stable-linux-amd64.zip -d {NG_DIR}\n",
    "\n",
    "# If tensorboard is alredy running kill it and restart with the correct logdir\n",
    "tb_pid = get_process_pid('tensorboard')\n",
    "if tb_pid:\n",
    "    print(\"Killing old tensorboard\")\n",
    "    os.kill(int(tb_pid), signal.SIGKILL)\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "tb_pid = get_process_pid('tensorboard')\n",
    "print (\"Started tensorboard with pid %s\" % tb_pid)\n",
    "\n",
    "# If ngrok is alredy running do nothing\n",
    "ng_pid = get_process_pid('ngrok')\n",
    "if not ng_pid:\n",
    "    proc = subprocess.Popen(['%s/ngrok' % NG_DIR , 'http', '6006'])\n",
    "    print (\"Started ngrok with pid %s\" % proc.pid)\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print (\"ngrok alredy runing\")\n",
    "ng_pid = get_process_pid('ngrok')\n",
    "\n",
    "# Get ngrok link\n",
    "try:\n",
    "    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "except:\n",
    "    print(\"Error getting ngrok link. Retrying...\")\n",
    "    time.sleep(5)\n",
    "    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "#procs = [tb_pid, ng_pid]\n",
    "#[os.kill(int(x), signal.SIGKILL) for x in procs if x is not None]\n",
    "#!rm -rf mlp-tb-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "Now try experimenting with the model. What effects do you see when changing the model parameters?\n",
    " - learning_rate\n",
    " - training_epochs\n",
    " - batch_size\n",
    " - n_hidden_1\n",
    " - n_hidden_2\n",
    "\n",
    "Try adding an additional hidden layer to the model. What impact does this have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of MLP Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
