{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow code for a Convoloutional Neural Network\n",
    "In this section we will go through the code for Convoloutional Neural Network in TensorFlow.\n",
    "\n",
    "Built around the implementation by [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we set up the required imports and define the location of the mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../scratch/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the relevant training and network parameters.\n",
    "The batch size given is 128. That means that each time, at most 128 images are fed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001 # Initial learning rate\n",
    "training_epochs = 2000 # Number of epochs to train\n",
    "batch_size = 128 # Number of images per batch\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "In the following `conv_net` definition we have 5 types of layers.\n",
    "\n",
    "1. convolution layers,\n",
    "2. max pooling layers,\n",
    "3. layers for reshaping input,\n",
    "4. fully-connected layers and\n",
    "5. dropout layers.\n",
    "\n",
    "#### conv2d layer\n",
    "\n",
    "A convolution layer tries to extract higher-level features by replacing data for each individual pixel with a value computed from the pixels covered by the specified filter centered on that pixel. We slide the filter across the width and height of the input image and compute the dot products between the pixels within the filter and input at each position.\n",
    "\n",
    "#### max_pooling2d layer\n",
    "\n",
    "Pooling layers reduce the overall size of the output by replacing values in the input by a function of those values. For example in max pooling we take the maximum out of every pool, such as 2x2,  as the new value for that pool.\n",
    "\n",
    "#### reshape layer\n",
    "\n",
    "There are two layers in this model that reshape the data. The first takes the imput data and reshapes it from a 1D array to a 3D structure, of (Height x Width x Channel), and the second reverses this process.\n",
    "\n",
    "#### Fully Connected (FC) layer\n",
    "\n",
    "The fully connected layer does a Wx + b for each neuron (number of neurons = number of outputs). Multiply each input by a weight, sum all those results up and then add the bias to get the output for that neuron.\n",
    "\n",
    "#### Dropout Layer\n",
    "\n",
    "Dropout sets a fraction of outputs passed on to the next layer to zero. These zeroed outputs are chosen randomly. This reduces overfitting by ensuring that the network can provide the right output even if some neurons are dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    sc_name = 'train' if is_training else 'test'\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse, default_name=sc_name+'_scope'):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1], name='Reshape')\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu, name='Conv_1')\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2, name='Pool_1')\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu, name='Conv_2')\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2, name='Pool_2')\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024, name='fc_layer')\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training, name='fc1_dropout')\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes, name='out_layer')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model function, loss and optimizer\n",
    "\n",
    "Define the model function using the tensorflow estimator template.\n",
    "\n",
    "NOTE: As we do not want any dropout during testing we specify a computation graph for each train and test, controlling the dropout with the `is_training` papameter.\n",
    "\n",
    "We also define:\n",
    "\n",
    "`tf.reduce_mean` - Computes the mean of elements across dimensions of a tensor.\n",
    "\n",
    "`tf.train.AdamOptimizer` - Adam optimiser provides an adaptive gradient algorithm.\n",
    "\n",
    "`optimizer.minimize` - Takes care of both computing the gradients and applying them with respect to `loss_op`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False,\n",
    "                            is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True,\n",
    "                           is_training=False)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1, name='pred_classes')\n",
    "    pred_probas = tf.nn.softmax(logits_test, name='pred_probas')\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)), name='loss_op')\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step(), name='train_op')\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes, name='acc_op')\n",
    "\n",
    "    # Include additional scalar for training accuracy\n",
    "    tf.summary.scalar('accuracy_train', acc_op[1])\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy_eval': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we construct a model object, passing the required inputs, and the tensorboard output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "output_dir = os.path.join( os.getcwd(),\"cnn-tb-\" + str(datetime.fromtimestamp(time())) )\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn, model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input function for training, which returns a function that will feed a dictionary of numpy arrays into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(input_fn, steps=training_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "eval = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", eval['accuracy_eval'])\n",
    "\n",
    "labels = list(mnist.test.labels)\n",
    "raw_predictions = model.predict(input_fn)\n",
    "predictions = (list(raw_predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "# Build confusion matrix from ground truth labels and model predictions\n",
    "conf_mat = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "%matplotlib inline\n",
    "# Plot matrix\n",
    "plt.matshow(conf_mat)\n",
    "plt.colorbar()\n",
    "plt.ylabel('Real Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tensorboard using an ngrok tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import signal\n",
    "\n",
    "def get_process_pid(pstring):\n",
    "    pid = None\n",
    "    for line in os.popen(\"ps ax | grep \" + pstring + \" | grep -v grep | grep -v defunct\"):\n",
    "        fields = line.split()\n",
    "        pid = fields[0]\n",
    "    return pid\n",
    "\n",
    "LOG_DIR = os.getcwd()\n",
    "NG_DIR = LOG_DIR\n",
    "# Uncomment if running locally\n",
    "#NG_DIR = os.path.dirname(LOG_DIR)\n",
    "NG_ZIP = os.path.join(NG_DIR, 'ngrok-stable-linux-amd64.zip')\n",
    "NG_BIN = os.path.join(NG_DIR, 'ngrok')\n",
    "\n",
    "# Download ngrok binary\n",
    "if not os.path.isfile(NG_ZIP):\n",
    "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip \\\n",
    "        -P {NG_DIR}\n",
    "if not os.path.isfile(NG_BIN):        \n",
    "    !unzip -o {NG_DIR}/ngrok-stable-linux-amd64.zip -d {NG_DIR}\n",
    "\n",
    "# If tensorboard is alredy running kill it and restart with the correct logdir\n",
    "tb_pid = get_process_pid('tensorboard')\n",
    "if tb_pid:\n",
    "    print(\"Killing old tensorboard\")\n",
    "    os.kill(int(tb_pid), signal.SIGKILL)\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "tb_pid = get_process_pid('tensorboard')\n",
    "print (\"Started tensorboard with pid %s\" % tb_pid)\n",
    "\n",
    "# If ngrok is alredy running do nothing\n",
    "ng_pid = get_process_pid('ngrok')\n",
    "if not ng_pid:\n",
    "    proc = subprocess.Popen(['%s/ngrok' % NG_DIR , 'http', '6006'])\n",
    "    print (\"Started ngrok with pid %s\" % proc.pid)\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print (\"ngrok alredy runing\")\n",
    "ng_pid = get_process_pid('ngrok')\n",
    "\n",
    "# Get ngrok link\n",
    "try:\n",
    "    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "except:\n",
    "    print(\"Error getting ngrok link. Retrying...\")\n",
    "    time.sleep(5)\n",
    "    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "#procs = [tb_pid, ng_pid]\n",
    "#[os.kill(int(x), signal.SIGKILL) for x in procs if x is not None]\n",
    "#!rm -rf cnn-tb-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "Now try experimenting with the model. What effects do you see when changing the model parameters?\n",
    " - learning_rate\n",
    " - training_epochs\n",
    " - batch_size\n",
    " - dropout\n",
    "\n",
    "Try adding an extra convolutional layer to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of CNN Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
